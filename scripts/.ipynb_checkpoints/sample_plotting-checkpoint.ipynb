{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Move to top of ArtifactNet directory so relative imports can be used here\n",
    "sys.path.insert(0, \"/cluster/home/carrowsm/ArtifactNet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Sample Images\n",
    "Use this notebook to load different models and pass sample images to them.\n",
    "\n",
    "## Define functions to load the PyTorch-Lightning models and the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(module: pl.LightningModule, checkpoint_path: str) :\n",
    "    model = module.load_from_checkpoint(PATH)\n",
    "    print(model.learning_rate)\n",
    "    # prints the learning_rate you used in this checkpoint\n",
    "\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataloaders\n",
    "from data.data_loader import PairedDataset, UnpairedDataset, load_image_data_frame\n",
    "from data.transforms import AffineTransform, ToTensor, Normalize\n",
    "\n",
    "# Load data frames with image labels\n",
    "val_x_df = pd.read_csv(\"/cluster/home/carrowsm/ArtifactNet/datasets/reza_test_X.csv\", dtype=str)\n",
    "val_x_df.set_index(\"patient_id\", inplace=True)\n",
    "val_y_df = pd.read_csv(\"/cluster/home/carrowsm/ArtifactNet/datasets/reza_test_Y.csv\", dtype=str)\n",
    "val_y_df.set_index(\"patient_id\", inplace=True)\n",
    "\n",
    "csv_path = \"/cluster/home/carrowsm/ArtifactNet/datasets/train_labels.csv\"\n",
    "y_df, n_df = load_image_data_frame(csv_path, [\"2\"], [\"1\"])\n",
    "\n",
    "# Save paths to data\n",
    "img_dir = \"/cluster/projects/radiomics/RADCURE-images/\"\n",
    "cache_dir = \"/cluster/projects/radiomics/Temp/colin/isotropic_nrrd/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data successfully cached\n",
      "\n",
      "\n",
      "Data successfully cached\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define sequence of transforms\n",
    "trg_transform = torchvision.transforms.Compose([\n",
    "                    AffineTransform(max_angle=20.0, max_pixels=[20, 20]), \n",
    "                    Normalize(-1000.0, 1000.0),\n",
    "                    ToTensor()])\n",
    "val_transform = torchvision.transforms.Compose([\n",
    "                    ToTensor()])\n",
    "\n",
    "# Initialize the two dataloaders\n",
    "trg_dataset = UnpairedDataset(y_df, n_df,\n",
    "                              image_dir=img_dir,\n",
    "                              cache_dir=cache_dir,\n",
    "                              file_type=\"DICOM\",\n",
    "                              image_size=[8, 256,256],\n",
    "                              dim=3,\n",
    "                              transform=trg_transform,\n",
    "                              num_workers=5)\n",
    "val_dataset = PairedDataset(val_x_df, val_x_df,\n",
    "                            image_dir=img_dir,\n",
    "                            cache_dir=os.path.join(cache_dir, \"paired_test\"),\n",
    "                            file_type=\"DICOM\",\n",
    "                            image_size=[8, 256,256],\n",
    "                            dim=3,\n",
    "                            transform=val_transform,\n",
    "                            num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          ...,\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],\n",
       "\n",
       "         [[-0.4528, -0.4530, -0.4589,  ..., -0.4596, -0.4588, -0.4515],\n",
       "          [-0.4515, -0.4550, -0.4566,  ..., -0.4614, -0.4583, -0.4522],\n",
       "          [-0.4585, -0.4577, -0.4543,  ..., -0.4580, -0.4533, -0.4554],\n",
       "          ...,\n",
       "          [-0.4572, -0.4553, -0.4514,  ..., -0.4514, -0.4545, -0.4548],\n",
       "          [-0.4551, -0.4518, -0.4509,  ..., -0.4479, -0.4507, -0.4575],\n",
       "          [-0.4474, -0.4489, -0.4493,  ..., -0.4473, -0.4483, -0.4518]],\n",
       "\n",
       "         [[-0.4922, -0.4924, -0.4971,  ..., -0.4987, -0.4974, -0.4906],\n",
       "          [-0.4901, -0.4936, -0.4951,  ..., -0.5000, -0.4969, -0.4918],\n",
       "          [-0.4956, -0.4958, -0.4929,  ..., -0.4970, -0.4925, -0.4941],\n",
       "          ...,\n",
       "          [-0.4958, -0.4945, -0.4907,  ..., -0.4869, -0.4902, -0.4910],\n",
       "          [-0.4940, -0.4916, -0.4894,  ..., -0.4850, -0.4869, -0.4919],\n",
       "          [-0.4879, -0.4890, -0.4891,  ..., -0.4859, -0.4851, -0.4872]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.5261, -0.5099, -0.5379,  ..., -0.5168, -0.4989, -0.5313],\n",
       "          [-0.5461, -0.5222, -0.5145,  ..., -0.5076, -0.5513, -0.5829],\n",
       "          [-0.5484, -0.5488, -0.5237,  ..., -0.5569, -0.5880, -0.5566],\n",
       "          ...,\n",
       "          [-0.5213, -0.5161, -0.5493,  ..., -0.5219, -0.5224, -0.5093],\n",
       "          [-0.5181, -0.5546, -0.5431,  ..., -0.5488, -0.5291, -0.5092],\n",
       "          [-0.5502, -0.5486, -0.5056,  ..., -0.5431, -0.5635, -0.5324]],\n",
       "\n",
       "         [[-0.5335, -0.5349, -0.5396,  ..., -0.5411, -0.5407, -0.5350],\n",
       "          [-0.5314, -0.5349, -0.5370,  ..., -0.5432, -0.5398, -0.5363],\n",
       "          [-0.5373, -0.5365, -0.5346,  ..., -0.5403, -0.5365, -0.5388],\n",
       "          ...,\n",
       "          [-0.5383, -0.5387, -0.5361,  ..., -0.5332, -0.5334, -0.5349],\n",
       "          [-0.5365, -0.5344, -0.5352,  ..., -0.5322, -0.5309, -0.5368],\n",
       "          [-0.5316, -0.5344, -0.5344,  ..., -0.5356, -0.5328, -0.5338]],\n",
       "\n",
       "         [[-0.6799, -0.6819, -0.6847,  ..., -0.6856, -0.6855, -0.6821],\n",
       "          [-0.6790, -0.6820, -0.6829,  ..., -0.6862, -0.6851, -0.6826],\n",
       "          [-0.6828, -0.6823, -0.6809,  ..., -0.6840, -0.6819, -0.6833],\n",
       "          ...,\n",
       "          [-0.6832, -0.6826, -0.6815,  ..., -0.6791, -0.6799, -0.6808],\n",
       "          [-0.6824, -0.6812, -0.6817,  ..., -0.6779, -0.6780, -0.6824],\n",
       "          [-0.6795, -0.6814, -0.6806,  ..., -0.6803, -0.6790, -0.6804]]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val1X, val1Y = val_dataset[0]\n",
    "val2X, val2Y = val_dataset[1]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAnet",
   "language": "python",
   "name": "danet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
