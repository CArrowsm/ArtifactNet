{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import multiprocessing\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import SimpleITK as sitk\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Move to top of ArtifactNet directory so relative imports can be used here\n",
    "sys.path.insert(0, \"/cluster/home/carrowsm/ArtifactNet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring and Testing ArtifactNet\n",
    "Use this notebook to load different models and pass sample images to them.\n",
    "\n",
    "## Define the dataloaders and CSVs containing the labels\n",
    "ArtifactNet requires a CSV for each dataset which labels the DA class of the images. In the examples below the class labels are `\"2\"`, `\"1\"`, and `\"0\"` for 'strong', 'weak', and 'none' respectively. These can be customized, as the actual labels are passed to the `load_image_data_frame` function which in turn selects the appropriate rows from the CSV. The data loading tools used in ArtifactNet are described in more detail below.\n",
    "\n",
    "### The load_image_data_frame() function\n",
    "This is a function which takes a dataframe containing the names of the image files (without file extensions) and the corresponding DA status label. The CSV is loaded into a pandas dataframe and the rows are grouped according to their DA status to form an unpaired set of images in domain $X$ and $Y$. These groups are then randomly split to form a training and validation set, with equal proportions of images from domain $X$ and $Y$ in each set.\n",
    "\n",
    "The function must be given a path to a CSV and the labels (as they appear in the CSV) to use for each domain. For example, if we want to create a data set containing 'strong' or 'weak' DA images in domain $X$ and only no-DA images in domain $Y$, we would write\n",
    "```\n",
    "x_trg, x_val, y_trg, y_val = load_image_data_frame(csv_path, [\"2\", \"1\"], [\"0\"])\n",
    "```\n",
    "To get pandas dataframes contaning the file names for images in the two domains of the training and validation set. To get the actual images, we have to pass these to a DataLoader object.\n",
    "\n",
    "### The Data Loaders: PairedDataset, UnpairedDataset\n",
    "These take the dataframes created by `load_image_data_frame()` and generate the actual pytorch datasets. `UnpairedDataset` is instantiated by passing it the data frames for domain $X$ and $Y$. This means each train, val, and test set requires defining its own dataset instance.\n",
    "\n",
    "Once instantiated, both `UnpairedDataset` and `PairedDataset` will create a cache of images preprocessed and cropped to the model input size and voxel spacing. If such a cache already exists, they will skip this step. When called, `UnpairedDataset` will return an image from domain $X$ and a randomly selected image from domain $Y$. Conversely, `PairedDataset` will return an image from $X$ and the paired image from $Y$, according to the ordering of the CSVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataloaders\n",
    "from data.data_loader import PairedDataset, UnpairedDataset, load_image_data_frame\n",
    "from data.transforms import AffineTransform, ToTensor, Normalize, HorizontalFlip\n",
    "\n",
    "# Locations of CSVs containing relevant info about the scans (e.g. DA status)\n",
    "phantom_csv = \"/cluster/home/carrowsm/ArtifactNet/datasets/phantoms.csv\"\n",
    "csv_path = \"/cluster/home/carrowsm/ArtifactNet/datasets/train_labels.csv\"\n",
    "\n",
    "# Load data frames with image labels\n",
    "x_df, _, y_df, _ = load_image_data_frame(csv_path, [\"2\", \"1\"], [\"0\"], val_split=0)\n",
    "val_x_df, _, val_y_df, _ = load_image_data_frame(phantom_csv, [\"2\", \"1\"], [\"0\"], val_split=0)\n",
    "\n",
    "# Where to read the original full scans from (img_dir) and \n",
    "# where to save/read a cache of preprocessed images (cache_dir)\n",
    "img_dir = \"/cluster/projects/radiomics/RADCURE-images/\"\n",
    "cache_dir = \"/cluster/projects/radiomics/Temp/colin/cyclegan_data/2-1-1mm_nrrd/\"\n",
    "phantom_dir = \"/cluster/projects/radiomics/Temp/colin/cyclegan_data/phantom_img/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some stats for the datasets\n",
    "print(\"DATA STATS\\n----------\")\n",
    "print( f\"DA+ images in training set: {len(x_df)}\")\n",
    "print(f\"DA- images in training set: {len(y_df)}\")\n",
    "print(f\"Number of image pairs in phantom set: {len(val_x_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data features\n",
    "n_cpus = 4                            # Number of cores to use for data loading\n",
    "img_shape = [16, 256, 256]             # Size of image for the model (in pixels)\n",
    "voxel_spacing = [2.0, 1.0, 1.0]        # Physical spacing between 3D pixels (mm)\n",
    "\n",
    "# Define sequence of transforms\n",
    "trg_transform = torchvision.transforms.Compose([\n",
    "#                             HorizontalFlip(),\n",
    "#                             AffineTransform(max_angle=30.0, max_pixels=[20, 20]),\n",
    "                            Normalize(-1000.0, 1000.0),\n",
    "                            ToTensor()])\n",
    "val_transform = torchvision.transforms.Compose([\n",
    "                            Normalize(-1000.0, 1000.0),\n",
    "                            ToTensor()])\n",
    "\n",
    "# Initialize the two dataloaders\n",
    "trg_dataset = UnpairedDataset(x_df, y_df,\n",
    "                              image_dir=img_dir,\n",
    "                              cache_dir=os.path.join(cache_dir, \"unpaired\"),\n",
    "                              file_type=\"DICOM\",\n",
    "                              image_size=img_shape,\n",
    "                              image_spacing=voxel_spacing,\n",
    "                              dim=3,\n",
    "                              transform=trg_transform,\n",
    "                              num_workers=n_cpus)\n",
    "val_dataset = PairedDataset(val_x_df, val_y_df,\n",
    "                            image_dir=phantom_dir,\n",
    "                            cache_dir=os.path.join(phantom_dir, \"paired\"),\n",
    "                            file_type=\"nrrd\",\n",
    "                            image_size=img_shape,\n",
    "                            image_spacing=voxel_spacing,\n",
    "                            dim=3,\n",
    "                            transform=val_transform,\n",
    "                            num_workers=n_cpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot some sample training and validation images\n",
    "We can load an image simply by calling the dataloader object. The returned image will be a pytorch tensor from each domain $X$ and $Y$. The 3D images will have a size defined by `img_shape` and should be centered on the slice with the strongest artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg1X, trg1Y = trg_dataset[16]\n",
    "val1X, val1Y = val_dataset[0]\n",
    "val2X, val2Y = val_dataset[1]\n",
    "val3X, val3Y = val_dataset[2]\n",
    "\n",
    "cm = \"Greys_r\"\n",
    "z_index = img_shape[0] // 2\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=[8, 8], facecolor='white')\n",
    "img1 = ax[0,0].imshow(trg1X[0, z_index, :, :], cmap=cm)\n",
    "ax[0,0].set_title(\"Original Strong DA image\")\n",
    "fig.colorbar(img1, ax=ax[0,0], shrink=0.6)\n",
    "\n",
    "img2 = ax[0,1].imshow(trg1Y[0, z_index, :, :], cmap=cm)\n",
    "ax[0,1].set_title(\"Original Weak DA image\")\n",
    "fig.colorbar(img2, ax=ax[0,1], shrink=0.6)\n",
    "\n",
    "img3 = ax[1,0].imshow(val2X[0, z_index, :, :], cmap=cm)\n",
    "ax[1,0].set_title(\"Phantom DA+ image\")\n",
    "fig.colorbar(img3, ax=ax[1,0], shrink=0.6)\n",
    "\n",
    "img4 = ax[1,1].imshow(val2Y[0, z_index, :, :], cmap=cm)\n",
    "ax[1,1].set_title(\"Phantom DA- image\")\n",
    "fig.colorbar(img4, ax=ax[1,1], shrink=0.6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "Use a pretrained ArtifactNet model to clean some images. We can use the paired phantom images to test how well the cleaning worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(module: pl.LightningModule, checkpoint_path: str) :\n",
    "    \"\"\"This function loads a model checkpoint (pretrained model) and \n",
    "    returns the frozen model parameters as a PyTorch-Lightning module.\n",
    "    \"\"\"\n",
    "    model = module.load_from_checkpoint(checkpoint_path)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model\n",
    "from cycleGAN import GAN\n",
    "checkpoint_path=\"/cluster/home/carrowsm/logs/cycleGAN/16_256_256px\\\n",
    "/2u1-0/version_0/checkpoints/epoch=62.ckpt\"\n",
    "\n",
    "if torch.cuda.is_available() :\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    print(f\"{n_gpus} GPUs are available\")\n",
    "    device = torch.device('cuda')\n",
    "else :\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model = load_model(GAN, checkpoint_path)    \n",
    "model.g_y.to(device)\n",
    "generator = model.g_y          # We only want to use the X -> Y generator\n",
    "del model, GAN                 # Free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some \"clean\" test images\n",
    "### WARNING: THIS IS MEMORY INTENSIVE ###\n",
    "with torch.no_grad() :\n",
    "    # Forward pass through the generator\n",
    "    val1_gen_y = generator(val1X.unsqueeze(0).to(device)).to(torch.device('cpu'))\n",
    "    val2_gen_y = generator(val2X.unsqueeze(0).to(device)).to(torch.device('cpu'))\n",
    "    val3_gen_y = generator(val3X.unsqueeze(0).to(device)).to(torch.device('cpu'))\n",
    "    trg1_gen_y = generator(trg1X.unsqueeze(0).to(device)).to(torch.device('cpu'))\n",
    "    \n",
    "    # Move the outputs back to the CPU\n",
    "    val1_gen_y = val1_gen_y.to(torch.device('cpu'))\n",
    "    val2_gen_y = val2_gen_y.to(torch.device('cpu'))\n",
    "    val3_gen_y = val3_gen_y.to(torch.device('cpu'))\n",
    "    trg1_gen_y = trg1_gen_y.to(torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison between clean and original\n",
    "## Phantom Images\n",
    "fig, ax = plt.subplots(nrows=2, ncols=3, figsize=[8*2, 8], facecolor='white')\n",
    "img = ax[0,0].imshow(val1Y[0, z_index, :, :], cmap=cm)\n",
    "ax[0,0].set_title(\"Phantom 1 - Original DA-\")\n",
    "fig.colorbar(img, ax=ax[0,0], shrink=1)\n",
    "\n",
    "img = ax[0,1].imshow(val1X[0, z_index, :, :], cmap=cm)\n",
    "ax[0,1].set_title(\"Phantom 1 - Original DA+\")\n",
    "fig.colorbar(img, ax=ax[0,1], shrink=1)\n",
    "\n",
    "img = ax[0,2].imshow(val1_gen_y[0, 0, z_index, :, :].numpy(), cmap=cm)\n",
    "ax[0,2].set_title(\"Phantom 1 - Generated DA-\")\n",
    "fig.colorbar(img, ax=ax[0,2], shrink=1)\n",
    "\n",
    "img = ax[1,0].imshow(val2Y[0, z_index-1, :, :], cmap=cm)\n",
    "ax[1,0].set_title(\"Phantom 2 - Original DA-\")\n",
    "fig.colorbar(img, ax=ax[1,0], shrink=1)\n",
    "\n",
    "img = ax[1,1].imshow(val2X[0, z_index-1, :, :], cmap=cm)\n",
    "ax[1,1].set_title(\"Phantom 2 - Original DA+\")\n",
    "fig.colorbar(img, ax=ax[1,1], shrink=1)\n",
    "\n",
    "img = ax[1,2].imshow(val2_gen_y[0, 0, z_index-1, :, :].numpy(), cmap=cm)\n",
    "ax[1,2].set_title(\"Phantom 2 - Generated DA-\")\n",
    "fig.colorbar(img, ax=ax[1,2], shrink=1)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Real Patient Image\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=[8*2, 8], facecolor='white')\n",
    "img = ax[0].imshow(trg1X[0, z_index, :, :], cmap=cm)\n",
    "ax[0].set_title(\"Sample Patient - Original DA+\")\n",
    "fig.colorbar(img, ax=ax[0], shrink=0.78)\n",
    "\n",
    "img = ax[1].imshow(trg1_gen_y[0, 0, z_index, :, :].numpy(), cmap=cm)\n",
    "ax[1].set_title(\"Sample Patient - Generated DA-\")\n",
    "fig.colorbar(img, ax=ax[1], shrink=0.78)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val1Y = val1Y.reshape(1, 1, 16, 256, 256)\n",
    "val2Y = val2Y.reshape(1, 1, 16, 256, 256)\n",
    "val3Y = val3Y.reshape(1, 1, 16, 256, 256)\n",
    "val1X = val1X.reshape(1, 1, 16, 256, 256)\n",
    "val2X = val2X.reshape(1, 1, 16, 256, 256)\n",
    "val3X = val3X.reshape(1, 1, 16, 256, 256)\n",
    "mse = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Test how well the paired images were cleaned\n",
    "print( mse(val1X, val1Y), mse(val1_gen_y, val1Y) )\n",
    "print(val1Y.mean(), val1X.mean(), val1_gen_y.mean())\n",
    "print(val1Y.std(), val1X.std(), val1_gen_y.std())\n",
    "\n",
    "print(\"\\n\")\n",
    "print( mse(val2X, val2Y), mse(val2_gen_y, val2Y) )\n",
    "print(val2Y.mean(), val2X.mean(), val2_gen_y.mean())\n",
    "print(val2Y.std(), val2X.std(), val2_gen_y.std())\n",
    "\n",
    "print(\"\\n\")\n",
    "print( mse(val3X, val3Y), mse(val3_gen_y, val3Y) )\n",
    "print(val3Y.mean(), val3X.mean(), val3_gen_y.mean())\n",
    "print(val3Y.std(), val3X.std(), val3_gen_y.std())\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow((val2X - val2Y)[0, 0, z_index, :, :])\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow((val2_gen_y - val2Y)[0, 0, z_index, :, :])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing\n",
    "ArtifactNet has a PostProcessor object which is used to reinsert the subvolume cleaned by the network back into the full original SITK image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test post processor\n",
    "from data.postprocessing import PostProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"datasets/radcure_challenge_test.csv\"\n",
    "orig_img_dir = \"/cluster/projects/radiomics/RADCURE-images/\"\n",
    "out_img_dir = \".\"\n",
    "\n",
    "postprocess = PostProcessor(orig_img_dir, out_img_dir,\n",
    "                            input_spacing=voxel_spacing,\n",
    "                            output_spacing=\"orig\",\n",
    "                            input_file_type=\"dicom\",\n",
    "                            output_file_type=\"nrrd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = trg_dataset.x_ids[16]\n",
    "img_center_x = float(x_df.at[patient_id, \"img_center_x\"])\n",
    "img_center_y = float(x_df.at[patient_id, \"img_center_y\"])\n",
    "img_center_z = float(x_df.at[patient_id, \"img_center_z\"])\n",
    "postprocess(trg1_gen_y.reshape(16, 256, 256), patient_id, \n",
    "            [img_center_x, img_center_y, img_center_z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load recreated image\n",
    "img = sitk.ReadImage(f\"{patient_id}.nrrd\")\n",
    "ximg = sitk.GetArrayFromImage(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(trg1_gen_y.reshape(16, 256, 256).numpy()[8, :, :], aspect='equal', cmap=cm)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(ximg[::-1, 190, :], aspect='auto', cmap=cm)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(ximg[::-1, :, 256], aspect='auto', cmap=cm)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(ximg[int(x_df.at[patient_id, \"a_slice\"]), :, :], aspect='equal', cmap=cm)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check available datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many images can be used to test OAR segmentation\n",
    "oar_df = pd.read_csv(\"../datasets/test_fold_oar_1.csv\",usecols=[\"index\", \"0\"],index_col=\"index\")\n",
    "gtv_df = pd.read_csv(\"../datasets/test_fold_gtv_1.csv\",usecols=[\"index\", \"0\"],index_col=\"index\")\n",
    "da_test_df = pd.read_csv(\"../datasets/test_labels.csv\", dtype=str).set_index(\"patient_id\")\n",
    "da_train_df = pd.read_csv(\"../datasets/train_labels.csv\", dtype=str).set_index(\"patient_id\")\n",
    "\n",
    "# Make column with MRN as index in each df\n",
    "oar_df[\"mrn\"] = oar_df.loc[:, \"0\"].str[-12:-5]\n",
    "oar_df.set_index(\"mrn\", inplace=True)\n",
    "gtv_df[\"mrn\"] = gtv_df.loc[:, \"0\"].str[-12:-5]\n",
    "gtv_df.set_index(\"mrn\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# common_gtv = da_test_df[da_test_df.index.isin(gtv_df.index)]\n",
    "# common_oar = da_test_df[da_test_df.index.isin(oar_df.index)]\n",
    "# full_da = pd.concat([da_train_df, da_test_df])\n",
    "\n",
    "# full_oar = full_da[full_da.index.isin(oar_df.index)]\n",
    "# full_oar.to_csv(\"../datasets/oar_segment_imgs.csv\")\n",
    "\n",
    "# full_gtv = full_da[full_da.index.isin(gtv_df.index)]\n",
    "# full_gtv.to_csv(\"../datasets/gtv_segment_imgs.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAnet",
   "language": "python",
   "name": "danet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
